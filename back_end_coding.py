# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AIkNikDvdCfHT1_G37SYt3M9RYC4JEYY

**SENTIMENT ANALYSIS OF IPHONE7**

##### **Dataset**
"""

import pandas as pd

df = pd.read_csv("iphone_7.csv")

df.head()

df.info()

df.columns



"""##### **Feature Selection**

1. **Remove effectless features**
2. **Punctuation removal**
3. **Positive:1, Neutral:0, Negative:-1**
4. **Stopwords removal**
5. **Tokenizer**
"""

df.isna().sum()



df.head()

df.shape

df.isna().sum()

df1 = df.copy()

df1["Reviews"] = df1["Reviews"].fillna(method='ffill')

df1.isna().sum()

df1

df.isna().sum()

"""**Filling the text sector isn't a viable option, as ratings and review statements doesn't coalesce together nor make sense**

**Hence, dropping the missing values is an acceptable course of action. Doing so, doesn't have a regretting impact on the dataset too**
"""

df.isna().sum()

def feature_engineering(df):
  
  # 1. Feature Selection
  del df["Unnamed: 0"]
  del df["Customer_name"]
  del df["Reviewed_date"]
  del df["Review_title"]
  del df["Ratings"]
  df = df.dropna()
  # df['Ratings'].replace(to_replace=['5.0 out of 5 stars','4.0 out of 5 stars','3.0 out of 5 stars','2.0 out of 5 stars','1.0 out of 5 stars'], value=[1,1,0,-1,-1],inplace=True)
  # df.rename(columns={"Ratings":"Categories"}, inplace=True)

  # 2. Punctuation removal
  import string
  def remove_punctuations(text):
    for punctuation in string.punctuation:
        text = text.replace(punctuation, "")
    return text
  df["Punctuation removed"] = df["Reviews"].apply(remove_punctuations)
  
  # 3. Stopwords
  import nltk
  nltk.download('stopwords')
  from nltk.corpus import stopwords
  stop = set(nltk.corpus.stopwords.words("english"))
  
  def stopwords_fn(text):
    stop_words_removed = ' '.join([word for word in text.split() if word not in stop])
    return stop_words_removed
  df["Stops removed"] = df["Punctuation removed"].apply(stopwords_fn)

  # 4. Lemmatize
  from nltk.stem import WordNetLemmatizer
  nltk.download('wordnet')
  nltk.download('averaged_perceptron_tagger')
  lemmatizer  = WordNetLemmatizer()
  w_tokenizer = nltk.tokenize.WhitespaceTokenizer()

  def lemma_fn(text):
    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]
  df["Lemmized"] = df["Stops removed"].apply(lemma_fn)

  del df["Reviews"]
  df.rename(columns={"Lemmized":"Reviews"}, inplace=True)
  df['Reviews'] = [" ".join(review) for review in df['Reviews'].values]
  del df["Punctuation removed"]
  del df["Stops removed"]

  # 5. TextBlob
  import textblob
  from textblob import TextBlob
  
  def tb(text):
    out = TextBlob(text).sentiment.polarity
    if out == 0:
      op = 0
    elif out > 0:
      op = 1
    else:
      op = -1

    return op
  df["Ratings"] = df["Reviews"].apply(tb)

  return df



df = feature_engineering(df)

df.head()

df.isna().sum()

df.shape

# df = df.assign(new_column=pd.cut(df['Ratings'], 
#                                  bins=[0, 2, 3.5, 999], 
#                                  labels=['Negative', 'Neutral', 'Positive']))

df



"""##### **The Split**"""

X = df[["Reviews"]]
y = df["Ratings"]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)



"""##### **Balance Check**"""

from collections import Counter
Counter(y_train)

import matplotlib.pyplot as plt
plt.figure(figsize=(10,7))
plt.hist(y_train)

"""**As seen above, balancing the target labels is a must**"""



# Upsampling

train = pd.concat([X_train, y_train], axis = 1)
test  = pd.concat([X_test, y_test], axis = 1)

neutral_train = train[train["Ratings"]==0]
negtive_train = train[train["Ratings"]==-1]
postive_train = train[train["Ratings"]==1]

from sklearn.utils import resample
neutral_upsampled = resample(neutral_train, replace=True, n_samples=2417)
negtive_upsampled = resample(negtive_train, replace=True, n_samples=2417)

train_upsampled = pd.concat([neutral_upsampled, negtive_upsampled, postive_train])

train_upsampled.isna().sum()

Counter(train_upsampled["Ratings"])

plt.hist(train_upsampled["Ratings"])

"""**As seen above, training data is successfully balanced**

**Now, the split is conducted - predictor and target variable**
"""

train_upsampled.shape

# Train - Predictor Vs. Target

X_train = train_upsampled[["Reviews"]]
y_train = train_upsampled["Ratings"]



"""##### **Vectorization**"""

# Vectorization

from sklearn.feature_extraction.text import TfidfVectorizer

feature_extraction = TfidfVectorizer(min_df=1, lowercase = True)

X_train_features = feature_extraction.fit_transform(X_train["Reviews"])

X_test_features = feature_extraction.transform(X_test["Reviews"])

print(X_train_features)



"""##### **Model building phase**

##### **Linear Regression**
"""

from sklearn.linear_model import LinearRegression

lr = LinearRegression()

lr.fit(X_train_features, y_train)

# Prediction

y_pred = lr.predict(X_test_features)
y_pred = y_pred.round()

# Classification Metrics

lr.score(X_test_features, y_test)

from sklearn.metrics import accuracy_score

accuracy_score(y_pred, y_test)



"""##### **LogisticRegression**"""

from sklearn.linear_model import LogisticRegression

lor = LogisticRegression()

lor.fit(X_train_features, y_train)

# Prediction

y_pred = lor.predict(X_test_features)
y_pred = y_pred.round()

# Classification metrics

lor.score(X_test_features, y_test)

accuracy_score(y_pred, y_test)



"""##### **DecisionTreeClassifier**"""

from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier()

tree.fit(X_train_features, y_train)

# Prediction

y_pred = tree.predict(X_test_features)

# Classification metrics

tree.score(X_test_features, y_test)

accuracy_score(y_pred, y_test)



"""##### **KNeighborsClassifier**"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()

knn.fit(X_train_features, y_train)

# Prediction

y_pred = knn.predict(X_test_features)

# Classification metrics

knn.score(X_test_features, y_test)

accuracy_score(y_pred, y_test)



"""##### **RandomForestClassifier**"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()

rfc.fit(X_train_features, y_train)

# PREDICTION

y_pred = rfc.predict(X_test_features)

# CLASSIFICATION METRICS

rfc.score(X_test_features, y_test)

rfc.score(X_test_features, y_test)

print(rfc.max_depth)

"""**Hyperparameter tuning**"""

rf_model = RandomForestClassifier()

# Hyperparameter Tuning - RandomizedSearchCV

import numpy as np

from sklearn.model_selection import RandomizedSearchCV

n_estimators = [int(x) for x in np.linspace(start = 5 , stop = 15, num = 10)] # returns 10 numbers 
max_features = ['auto', 'log2']
max_depth = [int(x) for x in np.linspace(5, 10, num = 2)] 
max_depth.append(None)
bootstrap = [True, False]
param_distributions =  {'n_estimators': n_estimators,
                        'max_depth' : 15,
                        'max_features': max_features,
                        'max_depth': max_depth,
                        'bootstrap': bootstrap}

print(param_distributions)

rfc_random = RandomizedSearchCV(estimator=rf_model, 
                                param_distributions=param_distributions, 
                                n_iter = 20, 
                                scoring='neg_mean_absolute_error', 
                                cv = 3, verbose=2, random_state=42, n_jobs=-1, return_train_score=True)

rfc_random.get_params()

rfc_random.fit(X_train_features, y_train)

# Prediction

y_pred_rand = rfc_random.predict(X_test_features)

from sklearn.metrics import accuracy_score

accuracy_score(y_pred_rand, y_test)

# classification metrics

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("1. Accuracy Score of Test Dataset - ", accuracy_score(y_pred_rand, y_test))        #model.score(X_test_features, y_test)
print("\n")
print("2. Confusion Matrix - \n", confusion_matrix(y_pred_rand, y_test))
print("\n")
print("3. Classification_report - \n\n", classification_report(y_pred_rand, y_test))



"""##### **SupportVectorMachines**"""

from sklearn.svm import SVC

svm = SVC()

svm.fit(X_train_features, y_train)

# Prediction

y_pred = svm.predict(X_test_features)

# Classification Metrics

svm.score(X_test_features, y_test)

accuracy_score(y_pred, y_test)

"""**Hyperparameter tuning**"""

# Hyperparameter tuning - GridSearchCV

from sklearn.model_selection import GridSearchCV

param_grid = {
              'C' : [0.1, 1, 10, 100, 1000],
              'gamma' : [1, 0.1, 0.01, 0.001, 0.0001],
              'gamma' : ['scale', 'auto'],
              'kernel' : ['linear', 'rbf', 'poly', 'sigmoid', 'nonlinear']
              }

grid = GridSearchCV(svm, param_grid, refit=True, cv=3, verbose=0)   # n_jobs=

grid.fit(X_train_features, y_train)

grid.score(X_train_features, y_train)

grid.score(X_test_features, y_test)

# Prediction

y_pred = grid.predict(X_test_features)

accuracy_score(y_pred, y_test)



"""##### **AdaBoostClassifier**"""

from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier()

ada.fit(X_train_features, y_train)

# Prediction

y_pred = ada.predict(X_test_features)

# Classification Metrics

print("Train score of AdaBoostClassifier -", ada.score(X_train_features, y_train))
print("Test score of AdaBoostClassifier -", ada.score(X_test_features, y_test))

"""**Hyperparameter tuning**"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import GridSearchCV

param_grid = {"base_estimator__criterion" : ["gini", "entropy"],
              "base_estimator__splitter" :   ["best", "random"],
              "n_estimators" : [1, 2]
             }


DTC = DecisionTreeClassifier(random_state = 11)       #, max_features = "auto", class_weight = "auto",max_depth = None)

ABC = AdaBoostClassifier(base_estimator = DTC)

# run grid search
grid_ABC = GridSearchCV(ABC, param_grid=param_grid, scoring = 'roc_auc')

grid_ABC.fit(X_train_features, y_train)

# grid_ABC.score(X_train_features, y_train)





"""##### **GradientBoostingAlgorithm**"""

from sklearn.ensemble import GradientBoostingClassifier

grad = GradientBoostingClassifier()

grad.fit(X_train_features, y_train)

# Prediction

y_pred = grad.predict(X_test_features)

accuracy_score(y_pred, y_test)



"""##### **XGBClassifier**"""

import xgboost
from xgboost import XGBClassifier
xgb = XGBClassifier()

xgb.fit(X_train_features, y_train)

# Prediction

y_pred = xgb.predict(X_test_features)

# Classification metrics

xgb.score(X_test_features, y_test)



"""##### **Inferred model**

The model selected for deployment is - **RandomForestClassifier**
"""